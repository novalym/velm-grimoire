# Path: src/velm/archetypes/genesis/ai-agent-swarm.scaffold
# ---------------------------------------------------------
# =================================================================================
# == GNOSTIC ARCHETYPE: AI AGENT SWARM (V-Î©-HIVE-MIND-ULTIMA)                    ==
# =================================================================================
# @description: A legendary, production-grade distributed AI system. Features
#               FastAPI producer, resilient workers, Redis persistence, DLQ,
#               structured logging, and graceful shutdown.
# @category: Intelligence
# @tags: python, redis, fastapi, ai, agents, distributed, system
# @difficulty: Grand Architect
# @is_integration: false
# @dna: use_docker=true, use_poetry=true, use_redis=true
# =================================================================================

# --- I. THE ALTAR OF VARIABLES ---
$$ project_name = "hive-mind"
$$ author = "The Architect"
$$ redis_port = 6379

# Derived Gnosis
$$ project_slug = {{ project_name | slug }}
$$ package_name = {{ project_name | snake }}

# --- II. THE SCRIPTURE OF FORM ---
{{ project_slug }}/

    # [1] INFRASTRUCTURE & CONFIGURATION
    .gitignore:
        __pycache__/
        *.py[cod]
        *$py.class
        .venv/
        .env
        .DS_Store
        coverage/
        .pytest_cache/
        dist/
        build/

    README.md :: """
    # {{ project_name }} (The Hive Mind)

    > A distributed, resilient AI task orchestration engine.

    ## ðŸ—ï¸ The Architecture of the Swarm

    This system uses a **Producer-Consumer** pattern with a **Stateful Ledger**.

    1.  **The Producer (FastAPI):** Ingests tasks, assigns them a unique UUID, initializes their state in Redis (`job:{id}`), and pushes them to the `job_queue`.
    2.  **The Worker (Python):** A daemon that consumes from `job_queue`. It performs the cognitive work (simulated or real AI inference) and updates the state in Redis.
    3.  **The Dead Letter Queue (DLQ):** If a task fails repeatedly, it is moved to `job_queue:dlq` for forensic analysis, preventing poison-pill loops.

    ## ðŸš€ Ignition

    1.  **Configure:**
        ```bash
        cp .env.example .env
        ```
    2.  **Awaken:**
        ```bash
        make up
        ```
    3.  **Commune:**
        ```bash
        # Submit a thought
        curl -X POST http://localhost:8000/api/v1/jobs \
             -H "Content-Type: application/json" \
             -d '{"payload": "Analyze the sentiment of this text.", "priority": "high"}'

        # Check status
        curl http://localhost:8000/api/v1/jobs/{job_id}
        ```
    """

    .env.example:
        # System
        ENVIRONMENT=development
        LOG_LEVEL=INFO

        # Redis (The Nervous System)
        REDIS_HOST=redis
        REDIS_PORT={{ redis_port }}
        REDIS_DB=0
        QUEUE_NAME=hive_tasks
        DLQ_NAME=hive_tasks:dlq

        # Worker Configuration
        WORKER_CONCURRENCY=1
        MAX_RETRIES=3

        # Cognition (Optional)
        OPENAI_API_KEY=sk-placeholder

    # [2] THE ORCHESTRATION LAYER (DOCKER)
    docker-compose.yml:
        version: '3.8'

        services:
          redis:
            image: redis:7-alpine
            ports:
              - "{{ redis_port }}:6379"
            command: redis-server --save 60 1 --loglevel warning
            volumes:
              - redis_data:/data
            healthcheck:
              test: ["CMD", "redis-cli", "ping"]
              interval: 5s
              timeout: 3s
              retries: 5

          producer:
            build:
              context: .
              dockerfile: deploy/Dockerfile.producer
            ports:
              - "8000:8000"
            env_file: .env
            depends_on:
              redis:
                condition: service_healthy
            volumes:
              - ./src:/app/src

          worker:
            build:
              context: .
              dockerfile: deploy/Dockerfile.worker
            env_file: .env
            depends_on:
              redis:
                condition: service_healthy
            volumes:
              - ./src:/app/src
            deploy:
              replicas: 2 # Scale the cognitive capacity

        volumes:
          redis_data:

    # [3] DEPENDENCY MATRIX
    pyproject.toml :: """
    [tool.poetry]
    name = "{{ project_slug }}"
    version = "0.1.0"
    description = "A resilient, distributed AI agent swarm."
    authors = ["{{ author }}"]

    [tool.poetry.dependencies]
    python = "^3.11"
    fastapi = "^0.110.0"
    uvicorn = {extras = ["standard"], version = "^0.27.0"}
    redis = "^5.0.1"
    pydantic = "^2.6.1"
    pydantic-settings = "^2.2.1"
    structlog = "^24.1.0"
    tenacity = "^8.2.3"
    openai = "^1.12.0"
    python-dateutil = "^2.8.2"

    [tool.poetry.group.dev.dependencies]
    pytest = "^8.0.0"
    ruff = "^0.2.1"
    black = "^24.2.0"
    httpx = "^0.26.0"

    [build-system]
    requires = ["poetry-core"]
    build-backend = "poetry.core.masonry.api"

    [tool.ruff]
    line-length = 100
    target-version = "py311"
    """

    Makefile:
        .PHONY: help up down logs restart test lint

        help:
            @grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = ":.*?## "}; {printf "\033[36m%-30s\033[0m %s\n", $$1, $$2}'

        up: ## Ignite the swarm (background)
            docker-compose up --build -d

        down: ## Dissolve the swarm
            docker-compose down

        logs: ## Gaze into the collective consciousness
            docker-compose logs -f

        restart: down up ## Reincarnate the swarm

        test: ## Run the inquisition (tests)
            poetry run pytest

        lint: ## Adjudicate code purity
            poetry run ruff check .
            poetry run black --check .

    # [4] DEPLOYMENT ARTIFACTS
    deploy/
        Dockerfile.producer :: """
        FROM python:3.11-slim-bookworm as builder
        
        WORKDIR /app
        
        # System Vows
        ENV PYTHONUNBUFFERED=1 \
            PYTHONDONTWRITEBYTECODE=1 \
            PIP_NO_CACHE_DIR=1

        # Dependencies
        RUN apt-get update && apt-get install -y --no-install-recommends curl \
            && rm -rf /var/lib/apt/lists/*

        # Poetry
        RUN pip install poetry==1.7.1
        COPY pyproject.toml poetry.lock ./
        RUN poetry config virtualenvs.create false && poetry install --no-interaction --no-ansi --no-root --only main

        # Application
        COPY src/ /app/src/

        # Healthcheck
        HEALTHCHECK --interval=30s --timeout=5s --start-period=5s --retries=3 \
            CMD curl -f http://localhost:8000/health || exit 1

        CMD ["uvicorn", "src.producer.main:app", "--host", "0.0.0.0", "--port", "8000"]
        """

        Dockerfile.worker :: """
        FROM python:3.11-slim-bookworm as builder
        
        WORKDIR /app
        
        # System Vows
        ENV PYTHONUNBUFFERED=1 \
            PYTHONDONTWRITEBYTECODE=1 \
            PIP_NO_CACHE_DIR=1

        # Dependencies
        RUN pip install poetry==1.7.1
        COPY pyproject.toml poetry.lock ./
        RUN poetry config virtualenvs.create false && poetry install --no-interaction --no-ansi --no-root --only main

        # Application
        COPY src/ /app/src/

        # The Worker runs as a module
        CMD ["python", "-m", "src.worker.main"]
        """

    # [5] THE SOURCE CODE (THE SOUL)
    src/
        __init__.py :: ""

        # =========================================================================
        # == SHARED CORE (THE GNOSTIC SPINE)                                     ==
        # =========================================================================
        core/
            __init__.py :: ""

            config.py :: """
            from pydantic_settings import BaseSettings, SettingsConfigDict
            from typing import Optional

            class Settings(BaseSettings):
                # Identity
                APP_NAME: str = "{{ project_name }}"
                ENVIRONMENT: str = "development"
                LOG_LEVEL: str = "INFO"
                
                # Nervous System (Redis)
                REDIS_HOST: str = "localhost"
                REDIS_PORT: int = {{ redis_port }}
                REDIS_DB: int = 0
                QUEUE_NAME: str = "hive_tasks"
                DLQ_NAME: str = "hive_tasks:dlq"

                # Resilience
                MAX_RETRIES: int = 3
                RETRY_BACKOFF_BASE: float = 1.0

                # Cognition
                OPENAI_API_KEY: Optional[str] = None

                model_config = SettingsConfigDict(env_file=".env", env_file_encoding="utf-8", extra="ignore")

            settings = Settings()
            """

            schemas.py :: """
            from pydantic import BaseModel, Field, UUID4
            from typing import Optional, Dict, Any, Literal
            import uuid
            import time
            from datetime import datetime

            JobStatus = Literal["PENDING", "PROCESSING", "COMPLETED", "FAILED", "DEAD"]

            class JobRequest(BaseModel):
                """The Architect's Plea."""
                payload: str = Field(..., min_length=1, description="The input data for the agent.")
                priority: str = Field("normal", description="Execution priority.")
                metadata: Dict[str, Any] = Field(default_factory=dict)

            class JobEnvelope(BaseModel):
                """The Gnostic Vessel traversing the queue."""
                job_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
                payload: str
                status: JobStatus = "PENDING"
                created_at: float = Field(default_factory=time.time)
                retry_count: int = 0
                trace_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
                
                # Result carriers
                result: Optional[str] = None
                error: Optional[str] = None
                processed_at: Optional[float] = None
                worker_id: Optional[str] = None
            """

            logger.py :: """
            import structlog
            import logging
            import sys
            import os

            def configure_logging():
                """Configures structured JSON logging for the swarm."""
                
                renderer = structlog.processors.JSONRenderer() if os.getenv("ENVIRONMENT") != "local" \
                    else structlog.dev.ConsoleRenderer()

                structlog.configure(
                    processors=[
                        structlog.contextvars.merge_contextvars,
                        structlog.processors.add_log_level,
                        structlog.processors.TimeStamper(fmt="iso"),
                        structlog.processors.StackInfoRenderer(),
                        structlog.processors.format_exc_info,
                        renderer
                    ],
                    logger_factory=structlog.PrintLoggerFactory(),
                    wrapper_class=structlog.make_filtering_bound_logger(logging.INFO),
                    cache_logger_on_first_use=True,
                )
            
            # Global Logger Instance
            log = structlog.get_logger()
            """

            redis_client.py :: """
            import redis.asyncio as redis
            from .config import settings
            from .logger import log

            class RedisConnector:
                """The Synaptic Bridge."""
                
                _pool: redis.ConnectionPool = None

                @classmethod
                async def get_client(cls) -> redis.Redis:
                    if cls._pool is None:
                        log.info("redis.connect", host=settings.REDIS_HOST)
                        cls._pool = redis.ConnectionPool(
                            host=settings.REDIS_HOST,
                            port=settings.REDIS_PORT,
                            db=settings.REDIS_DB,
                            decode_responses=True,
                            max_connections=50
                        )
                    return redis.Redis(connection_pool=cls._pool)
                
                @classmethod
                async def close(cls):
                    if cls._pool:
                        await cls._pool.disconnect()
                        log.info("redis.disconnect")
            """

        # =========================================================================
        # == THE PRODUCER (THE MOUTH)                                            ==
        # =========================================================================
        producer/
            __init__.py :: ""

            main.py :: """
            from contextlib import asynccontextmanager
            from fastapi import FastAPI, HTTPException, status
            from fastapi.responses import JSONResponse
            
            from ..core.config import settings
            from ..core.schemas import JobRequest, JobEnvelope
            from ..core.logger import configure_logging, log
            from ..core.redis_client import RedisConnector

            @asynccontextmanager
            async def lifespan(app: FastAPI):
                configure_logging()
                log.info("producer.startup", env=settings.ENVIRONMENT)
                # Verify Redis Connection
                try:
                    r = await RedisConnector.get_client()
                    await r.ping()
                except Exception as e:
                    log.error("producer.boot_failed", error=str(e))
                    raise e
                yield
                await RedisConnector.close()
                log.info("producer.shutdown")

            app = FastAPI(title="{{ project_name }} Producer", lifespan=lifespan)

            @app.get("/health")
            async def health_check():
                return {"status": "vital", "service": "producer"}

            @app.post("/api/v1/jobs", status_code=status.HTTP_202_ACCEPTED)
            async def submit_job(request: JobRequest):
                """Inscribes a job into the Ledger and Queue."""
                try:
                    r = await RedisConnector.get_client()
                    
                    # 1. Forge the Vessel
                    envelope = JobEnvelope(payload=request.payload)
                    
                    # 2. Inscribe to Ledger (Persistence)
                    # We store the full state in a hash: job:{id}
                    ledger_key = f"job:{envelope.job_id}"
                    await r.set(ledger_key, envelope.model_dump_json(), ex=86400) # 24h TTL
                    
                    # 3. Push to Queue (The Stream of Will)
                    await r.lpush(settings.QUEUE_NAME, envelope.job_id)
                    
                    log.info("job.submitted", job_id=envelope.job_id, trace_id=envelope.trace_id)
                    
                    return {
                        "message": "Job accepted",
                        "job_id": envelope.job_id,
                        "status_url": f"/api/v1/jobs/{envelope.job_id}"
                    }
                    
                except Exception as e:
                    log.error("job.submission_failed", error=str(e))
                    raise HTTPException(status_code=500, detail="Internal Nexus Failure")

            @app.get("/api/v1/jobs/{job_id}")
            async def get_job_status(job_id: str):
                """Scries the Ledger for the status of a job."""
                r = await RedisConnector.get_client()
                raw = await r.get(f"job:{job_id}")
                
                if not raw:
                    raise HTTPException(status_code=404, detail="Job not found in the Akashic records.")
                
                return JobEnvelope.model_validate_json(raw)
            """

        # =========================================================================
        # == THE WORKER (THE BRAIN)                                              ==
        # =========================================================================
        worker/
            __init__.py :: ""

            brain.py :: """
            import asyncio
            import random
            from ..core.logger import log

            class CognitiveEngine:
                """The Simulation of Thought."""
                
                @staticmethod
                async def process(payload: str) -> str:
                    """
                    Simulates a complex AI inference.
                    In a real reality, this would call OpenAI/Anthropic/HuggingFace.
                    """
                    log.info("brain.thinking", payload_preview=payload[:20])
                    
                    # Simulate variable latency
                    delay = random.uniform(0.5, 2.0)
                    await asyncio.sleep(delay)
                    
                    # Simulate stochastic failure (Entropy)
                    if random.random() < 0.05:
                        raise ValueError("Simulated Neural Collapse")
                        
                    return f"Analysis Complete: '{payload}' contains {len(payload)} chars. Sentiment: GNOSTIC."
            """

            main.py :: """
            import asyncio
            import socket
            import signal
            import time
            from typing import Optional

            from ..core.config import settings
            from ..core.schemas import JobEnvelope
            from ..core.logger import configure_logging, log
            from ..core.redis_client import RedisConnector
            from .brain import CognitiveEngine

            WORKER_ID = f"worker-{socket.gethostname()}-{settings.ENVIRONMENT}"
            SHUTDOWN_EVENT = asyncio.Event()

            def handle_sigterm():
                log.warning("worker.signal_received", signal="SIGTERM")
                SHUTDOWN_EVENT.set()

            async def update_ledger(r, envelope: JobEnvelope):
                """Updates the persistent state of the job."""
                await r.set(f"job:{envelope.job_id}", envelope.model_dump_json(), ex=86400)

            async def process_job(r, job_id: str):
                """The Rite of Processing."""
                # 1. Fetch Job State
                raw_data = await r.get(f"job:{job_id}")
                if not raw_data:
                    log.error("worker.job_vanished", job_id=job_id)
                    return

                envelope = JobEnvelope.model_validate_json(raw_data)
                
                # 2. Mark Processing
                envelope.status = "PROCESSING"
                envelope.worker_id = WORKER_ID
                await update_ledger(r, envelope)
                
                log.info("worker.job_start", job_id=job_id)

                try:
                    # 3. Cognitive Work
                    result = await CognitiveEngine.process(envelope.payload)
                    
                    # 4. Success
                    envelope.status = "COMPLETED"
                    envelope.result = result
                    envelope.processed_at = time.time()
                    await update_ledger(r, envelope)
                    log.info("worker.job_success", job_id=job_id)

                except Exception as e:
                    # 5. Failure / Retry Logic
                    envelope.retry_count += 1
                    error_msg = str(e)
                    log.error("worker.job_error", job_id=job_id, error=error_msg, attempt=envelope.retry_count)
                    
                    if envelope.retry_count >= settings.MAX_RETRIES:
                        # 6. Dead Letter Queue
                        envelope.status = "DEAD"
                        envelope.error = f"Max retries reached. Last error: {error_msg}"
                        await update_ledger(r, envelope)
                        await r.lpush(settings.DLQ_NAME, envelope.model_dump_json())
                        log.error("worker.job_dead", job_id=job_id)
                    else:
                        # 7. Re-queue for retry (Back of queue)
                        envelope.status = "PENDING"
                        envelope.error = f"Retry {envelope.retry_count}: {error_msg}"
                        await update_ledger(r, envelope)
                        # Optional: Implement delayed retry using a sorted set (ZADD) for precision
                        await r.lpush(settings.QUEUE_NAME, job_id)

            async def main_loop():
                configure_logging()
                log.info("worker.ignited", worker_id=WORKER_ID)
                
                loop = asyncio.get_running_loop()
                loop.add_signal_handler(signal.SIGTERM, handle_sigterm)
                loop.add_signal_handler(signal.SIGINT, handle_sigterm)

                r = await RedisConnector.get_client()

                while not SHUTDOWN_EVENT.is_set():
                    try:
                        # Blocking pop with short timeout to check shutdown flag frequently
                        # Returns (queue_name, value)
                        packet = await r.brpop(settings.QUEUE_NAME, timeout=2)
                        
                        if packet:
                            _, job_id = packet
                            await process_job(r, job_id)
                            
                    except Exception as e:
                        log.error("worker.loop_error", error=str(e))
                        await asyncio.sleep(1)

                log.info("worker.draining")
                await RedisConnector.close()
                log.info("worker.offline")

            if __name__ == "__main__":
                asyncio.run(main_loop())
            """

# --- III. THE RITE OF COMPLETION ---
%% post-run
    # 1. Initialize Chronicle
    @if {{ use_git }} -> git init

    # 2. Summon the Dependencies
    @if {{ shell('which poetry') }} -> poetry install

    # 3. Proclamation
    proclaim: "The Distributed Mind has been forged."
    proclaim: "Configuration: [bold green]src/core/config.py[/bold green]"
    proclaim: "Next Steps:"
    proclaim: "  1. [bold cyan]cp .env.example .env[/bold cyan]"
    proclaim: "  2. [bold cyan]make up[/bold cyan] (Ignites Redis, Producer, Worker)"
    proclaim: "  3. [bold cyan]make logs[/bold cyan] (Watch the swarm think)"

%% on-heresy
    proclaim: "The forging of the Swarm faltered. Cleaning the sanctum..."
    rm -rf src/ deploy/ docker-compose.yml